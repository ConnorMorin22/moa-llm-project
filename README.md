# Mixture of Agents for Efficient LLM Inference

This project explores the use of Mixture of Agents (MoA) for improving large language model inference performance, inspired by the paper [An Empirical Study of Mixture of Agents in Language Model Inference](https://huggingface.co/papers/2507.22827).

## Objectives
- Compare multiple LLMs on reasoning tasks
- Implement token-level routing to select best outputs
- Evaluate against benchmarks like MMLU and GSM8K

## Project Structure

ğŸ“¦ moa-llm-project
â”œâ”€â”€ agents/            # Agents using different LLMs
â”œâ”€â”€ router/            # Logic for choosing best output
â”œâ”€â”€ evaluate/          # Scripts and metrics tracking
â”œâ”€â”€ app/               # (Optional) Streamlit/FastAPI interface
â”œâ”€â”€ data/              # Prompts, results
â”œâ”€â”€ requirements.txt   # Dependencies

## Setup
```bash
pip install -r requirements.txt

#### 2. ğŸ§ª Optional: Expand "Coming Soon"
Right now, â€œComing Soonâ€ is a bullet list â€” consider breaking it into a full section for visibility:

```markdown
## ğŸš§ Coming Soon

- âœ… Baseline agent comparison
- âœ… Token router implementation
- âœ… Evaluation results and dashboard
- âœ… Optional Streamlit or FastAPI UI
