# moa-llm-project
Mixture of Agents for Efficient LLM Inference: A Practical Implementation and Evaluation https://huggingface.co/papers/2507.22827

# Mixture of Agents for Efficient LLM Inference

This project explores the use of Mixture of Agents (MoA) for improving large language model inference performance, inspired by the paper [An Empirical Study of Mixture of Agents in Language Model Inference](https://huggingface.co/papers/2507.22827).

## Objectives
- Compare multiple LLMs on reasoning tasks
- Implement token-level routing to select best outputs
- Evaluate against benchmarks like MMLU and GSM8K

## Project Structure
